{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## data creating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data successfully converted to output.csv\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "# Define the path to your JSONL file\n",
    "jsonl_file_path = 'data.jsonl'\n",
    "# Define the path to the output CSV file\n",
    "csv_file_path = 'output.csv'\n",
    "\n",
    "# Read the JSONL file and store the data in a list\n",
    "data = []\n",
    "with open(jsonl_file_path, 'r') as file:\n",
    "    for line in file:\n",
    "        data.append(json.loads(line))\n",
    "\n",
    "# Convert the list of dictionaries to a pandas DataFrame\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Rename columns to match the required CSV format\n",
    "df.columns = ['input_text', 'response_text']\n",
    "\n",
    "# Save the DataFrame to a CSV file\n",
    "df.to_csv(csv_file_path, index=False)\n",
    "\n",
    "print(f\"Data successfully converted to {csv_file_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\mithi\\miniconda3\\envs\\face_rec\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                     input_text  \\\n",
      "0                  What is the price of the TV?   \n",
      "1                  Do you have any gas cookers?   \n",
      "2                              I need a fridge.   \n",
      "3  Can you tell me about your Samsung products?   \n",
      "4     Do you have any discounts on electronics?   \n",
      "\n",
      "                                       response_text  \n",
      "0       Sure, let me check the latest price for you.  \n",
      "1  Sure, let me see what gas cookers we have in s...  \n",
      "2        Let me check the available fridges for you.  \n",
      "3  I can provide you with the latest details on o...  \n",
      "4  Let me check if we have any discounts on elect...  \n",
      "Dataset({\n",
      "    features: ['input_text', 'response_text'],\n",
      "    num_rows: 71\n",
      "})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating CSV from Arrow format: 100%|██████████| 1/1 [00:00<00:00, 60.72ba/s]\n",
      "Generating train split: 71 examples [00:00, 5065.15 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['input_text', 'response_text'],\n",
      "        num_rows: 71\n",
      "    })\n",
      "})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from datasets import load_dataset, Dataset\n",
    "\n",
    "# Load the CSV file into a pandas DataFrame\n",
    "df = pd.read_csv('output.csv')\n",
    "\n",
    "# Display the DataFrame to check if the data is loaded correctly\n",
    "print(df.head())\n",
    "\n",
    "# Convert the DataFrame to a Dataset\n",
    "dataset = Dataset.from_pandas(df)\n",
    "\n",
    "# Display the Dataset to verify the conversion\n",
    "print(dataset)\n",
    "\n",
    "# You can now use this dataset to train your model.\n",
    "# For example, to save the dataset to a file:\n",
    "dataset.to_csv('prepared_customer_seller_data.csv', index=False)\n",
    "\n",
    "# To load the dataset later:\n",
    "loaded_dataset = load_dataset('csv', data_files='prepared_customer_seller_data.csv')\n",
    "\n",
    "# Display the loaded dataset to verify\n",
    "print(loaded_dataset)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## training model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 71/71 [00:00<00:00, 2268.49 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "An error occurred: Using the `Trainer` with `PyTorch` requires `accelerate>=0.21.0`: Please run `pip install transformers[torch]` or `pip install accelerate -U`\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from datasets import Dataset\n",
    "from transformers import Trainer, TrainingArguments, GPT2LMHeadModel, GPT2Tokenizer\n",
    "\n",
    "# Use the correct file path\n",
    "file_path = 'prepared_customer_seller_data.csv'\n",
    "\n",
    "try:\n",
    "    # Load the CSV file into a pandas DataFrame\n",
    "    df = pd.read_csv(file_path)\n",
    "    \n",
    "    # Combine input and response text for causal language modeling\n",
    "    df['text'] = df['input_text'] + \" \" + df['response_text']\n",
    "    \n",
    "    # Convert the DataFrame to a Dataset\n",
    "    dataset = Dataset.from_pandas(df[['text']])\n",
    "    \n",
    "    # Initialize the tokenizer and model\n",
    "    model_name = 'gpt2'  # Change to 'gpt2-small' if needed\n",
    "    tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
    "    model = GPT2LMHeadModel.from_pretrained(model_name)\n",
    "    \n",
    "    # Add a padding token if not already present\n",
    "    if tokenizer.pad_token is None:\n",
    "        tokenizer.pad_token = tokenizer.eos_token  # Use the end-of-sequence token as padding token\n",
    "\n",
    "    # Tokenize the combined text\n",
    "    def tokenize_function(example):\n",
    "        return tokenizer(example[\"text\"], padding=\"max_length\", truncation=True, max_length=512)\n",
    "    \n",
    "    tokenized_datasets = dataset.map(tokenize_function, batched=True, remove_columns=[\"text\"])\n",
    "    \n",
    "    # Prepare training arguments\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir='./results',\n",
    "        per_device_train_batch_size=1,\n",
    "        num_train_epochs=3,  # Adjust the number of epochs as needed\n",
    "        logging_dir='./logs',\n",
    "        logging_steps=10,\n",
    "        save_steps=500,\n",
    "        save_total_limit=2,\n",
    "    )\n",
    "    \n",
    "    # Initialize the Trainer\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=tokenized_datasets,\n",
    "    )\n",
    "    \n",
    "    # Train the model\n",
    "    trainer.train()\n",
    "    \n",
    "    # Save the model\n",
    "    model.save_pretrained('custom_seller_bot_model')\n",
    "    tokenizer.save_pretrained('custom_seller_bot_model')\n",
    "    \n",
    "    print(\"Training complete and model saved.\")\n",
    "    \n",
    "except FileNotFoundError:\n",
    "    print(f\"The file at {file_path} was not found. Please check the path and try again.\")\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 71/71 [00:00<00:00, 557.97 examples/s]\n",
      "  0%|          | 0/213 [03:29<?, ?it/s]\n",
      "  5%|▍         | 10/213 [03:08<36:24, 10.76s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.2613, 'grad_norm': 2.626744270324707, 'learning_rate': 4.765258215962441e-05, 'epoch': 0.14}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  9%|▉         | 20/213 [04:33<27:38,  8.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0983, 'grad_norm': 1.6586312055587769, 'learning_rate': 4.530516431924883e-05, 'epoch': 0.28}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 14%|█▍        | 30/213 [06:07<29:19,  9.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0612, 'grad_norm': 1.7735347747802734, 'learning_rate': 4.295774647887324e-05, 'epoch': 0.42}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 19%|█▉        | 40/213 [07:35<24:59,  8.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0396, 'grad_norm': 1.2576494216918945, 'learning_rate': 4.0610328638497654e-05, 'epoch': 0.56}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 23%|██▎       | 50/213 [09:03<23:25,  8.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0312, 'grad_norm': 1.0658440589904785, 'learning_rate': 3.826291079812207e-05, 'epoch': 0.7}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 28%|██▊       | 60/213 [10:30<22:19,  8.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0268, 'grad_norm': 0.5717741250991821, 'learning_rate': 3.5915492957746486e-05, 'epoch': 0.85}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|███▎      | 70/213 [11:55<20:10,  8.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0302, 'grad_norm': 1.3320704698562622, 'learning_rate': 3.3568075117370895e-05, 'epoch': 0.99}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 38%|███▊      | 80/213 [13:20<18:53,  8.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0155, 'grad_norm': 0.47770926356315613, 'learning_rate': 3.1220657276995305e-05, 'epoch': 1.13}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 42%|████▏     | 90/213 [14:44<17:25,  8.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0162, 'grad_norm': 1.3124406337738037, 'learning_rate': 2.887323943661972e-05, 'epoch': 1.27}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 47%|████▋     | 100/213 [16:09<16:05,  8.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0245, 'grad_norm': 0.5552031397819519, 'learning_rate': 2.6525821596244134e-05, 'epoch': 1.41}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 52%|█████▏    | 110/213 [17:33<15:34,  9.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0171, 'grad_norm': 0.6141884326934814, 'learning_rate': 2.4178403755868547e-05, 'epoch': 1.55}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 56%|█████▋    | 120/213 [18:57<12:53,  8.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0151, 'grad_norm': 0.5859781503677368, 'learning_rate': 2.1830985915492956e-05, 'epoch': 1.69}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 61%|██████    | 130/213 [20:21<11:30,  8.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0184, 'grad_norm': 0.5368008017539978, 'learning_rate': 1.9483568075117372e-05, 'epoch': 1.83}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 66%|██████▌   | 140/213 [21:46<10:32,  8.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0149, 'grad_norm': 0.4845516085624695, 'learning_rate': 1.7136150234741785e-05, 'epoch': 1.97}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|███████   | 150/213 [25:51<19:27, 18.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0164, 'grad_norm': 0.4224061071872711, 'learning_rate': 1.4788732394366198e-05, 'epoch': 2.11}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 75%|███████▌  | 160/213 [27:14<07:40,  8.70s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0129, 'grad_norm': 1.1524507999420166, 'learning_rate': 1.2441314553990612e-05, 'epoch': 2.25}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|███████▉  | 170/213 [28:38<05:49,  8.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0108, 'grad_norm': 0.6417281627655029, 'learning_rate': 1.0093896713615023e-05, 'epoch': 2.39}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 85%|████████▍ | 180/213 [30:00<04:29,  8.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.016, 'grad_norm': 0.4205755293369293, 'learning_rate': 7.746478873239436e-06, 'epoch': 2.54}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 89%|████████▉ | 190/213 [31:24<03:14,  8.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0103, 'grad_norm': 0.519544243812561, 'learning_rate': 5.3990610328638506e-06, 'epoch': 2.68}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 94%|█████████▍| 200/213 [32:47<01:47,  8.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.013, 'grad_norm': 0.6004200577735901, 'learning_rate': 3.051643192488263e-06, 'epoch': 2.82}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 99%|█████████▊| 210/213 [34:09<00:24,  8.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0192, 'grad_norm': 0.5160269737243652, 'learning_rate': 7.042253521126761e-07, 'epoch': 2.96}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 213/213 [35:02<00:00,  9.87s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 2102.3276, 'train_samples_per_second': 0.101, 'train_steps_per_second': 0.101, 'train_loss': 0.13021979012259854, 'epoch': 3.0}\n",
      "Training complete and model saved.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from datasets import Dataset\n",
    "from transformers import Trainer, TrainingArguments, GPT2LMHeadModel, GPT2Tokenizer\n",
    "\n",
    "# Use the correct file path\n",
    "file_path = 'prepared_customer_seller_data.csv'\n",
    "\n",
    "try:\n",
    "    # Load the CSV file into a pandas DataFrame\n",
    "    df = pd.read_csv(file_path)\n",
    "    \n",
    "    # Combine input and response text for causal language modeling\n",
    "    df['text'] = df['input_text'] + \" \" + df['response_text']\n",
    "    \n",
    "    # Convert the DataFrame to a Dataset\n",
    "    dataset = Dataset.from_pandas(df[['text']])\n",
    "    \n",
    "    # Initialize the tokenizer and model\n",
    "    model_name = 'gpt2'  # or 'gpt2-small', 'gpt2-medium', 'gpt2-large', etc.\n",
    "    tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
    "    model = GPT2LMHeadModel.from_pretrained(model_name)\n",
    "    \n",
    "    # Add a padding token if not already present\n",
    "    if tokenizer.pad_token is None:\n",
    "        tokenizer.pad_token = tokenizer.eos_token  # Use the end-of-sequence token as padding token\n",
    "\n",
    "    # Tokenize the combined text\n",
    "    def tokenize_function(example):\n",
    "        encodings = tokenizer(example[\"text\"], padding=\"max_length\", truncation=True, max_length=512)\n",
    "        encodings[\"labels\"] = encodings[\"input_ids\"].copy()  # Use input_ids as labels\n",
    "        return encodings\n",
    "    \n",
    "    tokenized_datasets = dataset.map(tokenize_function, batched=True, remove_columns=[\"text\"])\n",
    "    \n",
    "    # Prepare training arguments\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir='./results',\n",
    "        per_device_train_batch_size=1,\n",
    "        num_train_epochs=3,\n",
    "        logging_dir='./logs',\n",
    "        logging_steps=10,\n",
    "        save_steps=500,\n",
    "        save_total_limit=2,\n",
    "        report_to=None,  # Ensure you're not using any unsupported reporting options\n",
    "    )\n",
    "    \n",
    "    # Initialize the Trainer\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=tokenized_datasets,\n",
    "    )\n",
    "    \n",
    "    # Train the model\n",
    "    trainer.train()\n",
    "    \n",
    "    # Save the model\n",
    "    model.save_pretrained('custom_seller_bot_model')\n",
    "    tokenizer.save_pretrained('custom_seller_bot_model')\n",
    "    \n",
    "    print(\"Training complete and model saved.\")\n",
    "    \n",
    "except FileNotFoundError:\n",
    "    print(f\"The file at {file_path} was not found. Please check the path and try again.\")\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version: 2.3.1+cpu\n",
      "Model and tokenizer loaded successfully.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "\n",
    "# Check PyTorch version\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "\n",
    "# Initialize model and tokenizer\n",
    "model_name = 'gpt2'\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
    "model = GPT2LMHeadModel.from_pretrained(model_name)\n",
    "\n",
    "print(\"Model and tokenizer loaded successfully.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accelerate version: 0.33.0\n",
      "Transformers version: 4.43.4\n"
     ]
    }
   ],
   "source": [
    "import accelerate\n",
    "import transformers\n",
    "\n",
    "print(f\"Accelerate version: {accelerate.__version__}\")\n",
    "print(f\"Transformers version: {transformers.__version__}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "\n",
    "model_name = 'gpt2'  # or 'gpt2-small'\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
    "model = GPT2LMHeadModel.from_pretrained(model_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "face_rec",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
